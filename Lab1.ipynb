{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "title",
      "metadata": {},
      "source": [
        "# Scaling Laws with nanoGPT\n",
        "__Designed by Edouard Oyallon__\n",
        "\n",
        "In this lab, we’ll use **nanoGPT** (Karpathy’s minimal GPT training code) together with the character-level dataset **`shakespeare_char`** to explore **compute-optimal training**, inspired by **Chinchilla-style scaling** (see https://arxiv.org/abs/2203.15556 and Section 3 of https://arxiv.org/pdf/2401.02954).\n",
        "\n",
        "**Resources**\n",
        "- **nanoGPT repo:** https://github.com/karpathy/nanoGPT  \n",
        "- **Dataset:** `data/shakespeare_char` (character-level Shakespeare)\n",
        "\n",
        "---\n",
        "\n",
        "## Q0 — Setup\n",
        "\n",
        "Before we start tuning or running sweeps, make sure your environment is working end-to-end:\n",
        "\n",
        "- **Clone and explore** the nanoGPT repository (training loop, model definition, config system).\n",
        "- **Preprocess** the `shakespeare_char` dataset and confirm the dataset files are generated correctly.\n",
        "- **Run a tiny training job** (a few hundred iterations) and verify that:\n",
        "  - loss decreases,\n",
        "  - checkpoints/logs are written,\n",
        "  - you can reproduce the run from the proposed config.\n",
        "\n",
        "> Optional: you *can* integrate nanoGPT with **Weights & Biases** for logging and sweeps, but it’s not required for this lab (and we won’t rely on it)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b59fc16",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "823380a0",
      "metadata": {},
      "source": [
        "## Preliminary questions\n",
        "\n",
        "### Q1 — Hardware FLOPs budget\n",
        "\n",
        "Your first job is to quantify your **compute budget**. We’ll use this later to reason about *compute-optimal* choices (tokens vs parameters).\n",
        "\n",
        "**Peak throughput.**  \n",
        "   Find your GPU’s **peak FP16 throughput** (in TFLOP/s). Using that value, estimate how many FLOPs you can deliver in **5 minutes** at peak.  \n",
        "   - Report: GPU model, peak FP16 TFLOP/s, and total FLOPs in 5 minutes.  \n",
        "   - Then **update** nanoGPT’s `estimate_mfu` (in `model.py`) so it matches *your* hardware peak. MFU (in %) allows to quantify GPU utilization.\n",
        "\n",
        "\n",
        "Helpful reference (hardware specs): https://epoch.ai/data/machine-learning-hardware\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79042192",
      "metadata": {},
      "source": [
        "**Answer:** \n",
        "\n",
        "GPU : Nvidia RTX 4000 Ada\n",
        "Peak FP16 TFLOP/s - 19.2 TFLOP/s \n",
        "Total FLOPs in 5 minutes : 4.59 * 10^16 FLOPs "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff1de63",
      "metadata": {},
      "source": [
        "### Q2 — Hyperparameter methodology\n",
        "\n",
        "Take one minute and propose the **simplest possible methodology** to choose hyperparameters for the **best model you can train in 5 minutes on your GPU** (i.e., a plan you can realistically follow until the end of the lab).\n",
        "\n",
        "- What would you try **first** (and why)?\n",
        "- Roughly **how many runs** would you need before you trust your choice?\n",
        "- Is this approach **practical** given your time and compute constraints?\n",
        "\n",
        "*Answer briefly, but justify your reasoning.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6900d0e4",
      "metadata": {},
      "source": [
        "**Answer:** "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c076b5d",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "repo_tasks",
      "metadata": {},
      "source": [
        "### Q3 — Identify the key parameters in `train.py`\n",
        "\n",
        "For compute-optimal training, the important knobs are the ones that control:\n",
        "\n",
        "- **model scale** $N$ (parameter count / FLOPs per token),\n",
        "- **training tokens** $D$ (how many tokens you process),\n",
        "- **optimization stability** (so losses are comparable),\n",
        "- **compute efficiency** (throughput / MFU)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8b995c4",
      "metadata": {},
      "source": [
        "**Answer:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "414f28c7",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b056dfbd",
      "metadata": {},
      "source": [
        "### Q4 — Instrument nanoGPT (FLOPs + saved results)\n",
        "\n",
        "Update the nanoGPT codebase to support scaling-law experiments:\n",
        "\n",
        "- **Add** a function `estimate_flops_per_token()` in `model.py` that estimates the model’s **FLOPs per token** (for a forward+backward training step).\n",
        "- **At the end of training**, save a single file named **`output.pt`** inside `--out_dir` (using `torch.save`) that contains:\n",
        "  1. `best_train_loss`\n",
        "  2. `best_val_loss`\n",
        "  3. `model_args`\n",
        "  4. `config`\n",
        "  5. `flops_per_token`\n",
        "  6. `params_no_embed` (parameter count excluding token/position embeddings)\n",
        "\n",
        "*Hint:* look at nanoGPT’s `scaling_laws.ipynb` and the existing `estimate_mfu` code path for inspiration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "design_model",
      "metadata": {},
      "source": [
        "## Model design\n",
        "\n",
        "### Q5 — Minimal model configuration\n",
        "\n",
        "Because we assume limited / slow GPU resources, we will use a **tiny baseline model** by reducing the embedding size.\n",
        "\n",
        "Set:\n",
        "- `--n_layer=4`\n",
        "- `--n_head=2`\n",
        "- `--block_size=1024`\n",
        "\n",
        "What constraint does this impose on the embedding size `n_embd`?\n",
        "\n",
        "Then plot the number of FLOPs as a function of $N$, with:\n",
        "$$\n",
        "n_{\\text{embd}} = n_{\\text{embd,min}} \\cdot N\n",
        "$$\n",
        "and suggest a simple proxy to FLOPs-per-token. \n",
        "\n",
        "This will be our **minimal reference model** for the rest of the lab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f7715da",
      "metadata": {},
      "source": [
        "**Answer:** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f46f837",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "wrapper_task",
      "metadata": {},
      "source": [
        "## Job wrapper\n",
        "\n",
        "### Q6 — Write a bash wrapper for reproducible training runs\n",
        "\n",
        "Write a bash function `run_nanogpt_job` that launches `nanoGPT/train.py` as a function of:\n",
        "\n",
        "- `D`: total number of training tokens to process  \n",
        "- `lr`: learning rate  \n",
        "- `N`: a simple proxy for model scale (e.g., map `N -> N*n_embd`, while keeping `n_layer` and `n_head` fixed)\n",
        "\n",
        "Your function must:\n",
        "\n",
        "- compute `max_iters` so that the total processed tokens is approximately  \n",
        "  $$\n",
        "  D \\approx \\text{max\\_iters} \\times \\text{batch\\_size} \\times \\text{block\\_size}\n",
        "  $$\n",
        "  (assuming `gradient_accumulation_steps=1`)\n",
        "- set:\n",
        "  - `batch_size = 10`\n",
        "  - `block_size = 1024`\n",
        "  - `warmup_iters = 0.3% * max_iters`\n",
        "  - `lr_decay_iters = max_iters`\n",
        "  - `min_lr = lr / 10`\n",
        "- keep evaluation **disabled or negligible** during sweeps (e.g., `eval_interval=max_iters`)\n",
        "- write outputs to a unique run directory named with `lr`, `D`, and `N` and save logs there\n",
        "\n",
        "Test it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "example_run",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "source ./nanoGPTwrapper.sh\n",
        "\n",
        "ROOT=\"out/test/run_$(date +%Y%m%d_%H%M%S)\"\n",
        "\n",
        "# run_nanogpt_job <lr> <D_tokens> <N_model_scale> <root_out_dir> <job_idx>\n",
        "run_nanogpt_job 1e-3 100000 10 \"$ROOT\" 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lr_sweep_task",
      "metadata": {},
      "source": [
        "## Finding the right hyperparameters\n",
        "\n",
        "### Optimal learning rate\n",
        "\n",
        "We assume the **compute-optimal learning rate** follows a power law in the compute budget $C$:\n",
        "\n",
        "$$\n",
        "\\eta^\\star(C) = a_{\\mathrm{lr}}\\,C^{\\,b_{\\mathrm{lr}}}\n",
        "$$\n",
        "\n",
        "\n",
        "Following the DeepSeek approach, we will keep **all other hyperparameters fixed** and vary only the quantities of interest. For simplicity and to design experiments, we assume the training compute scales as\n",
        "\n",
        "$$\n",
        "C \\propto N\\,D,\n",
        "$$\n",
        "\n",
        "i.e., FLOPs per token scale like $N$ and total compute is proportional to $N$ times the number of processed tokens $D$.\n",
        "\n",
        "\n",
        "#### Q7 — Estimate $a_{\\mathrm{lr}}$ and $b_{\\mathrm{lr}}$\n",
        "\n",
        "Design and run a sweep of **10+ training experiments** spanning a reasonable range of compute budgets $C$.  \n",
        "For each budget, you should search over a small set of candidate learning rates and keep the one that achieves the **best validation loss**.\n",
        "\n",
        "Use the following midpoint as a reference scale:\n",
        "\n",
        "- $D_{\\text{mid}} = 1{,}250{,}000$  (tokens)\n",
        "- $N_{\\text{mid}} = 10$            (model scale proxy)\n",
        "\n",
        "\n",
        "Your goal is to collect pairs $\\big(C, \\eta^*(C)\\big)$ and fit $a_{\\mathrm{lr}}$ and $b_{\\mathrm{lr}}$ via a log–log regression.\n",
        "\n",
        "First, run the corresponding experiments and save all outputs under `out/lr`.   Use **one subfolder per experiment** (e.g., named with the key hyperparameters like `lr`, `D`, and `N`) so that each run is easy to identify and parse later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lr_sweep_bash",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "aggregate_task",
      "metadata": {},
      "source": [
        " Then, aggregate results from all runs by reading each `output.pt`. For every run:\n",
        "\n",
        "- extract the best validation loss,\n",
        "- group runs by $(N, D)$ iso-FLOPs,\n",
        "- identify the learning rate that achieves the **lowest** validation loss for each $(N, D)$.\n",
        "\n",
        "Finally, collect the resulting pairs $\\big(C, \\eta^\\star(C)\\big)$ (with $C \\propto ND$) for fitting the learning-rate scaling law."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "helpers_collect",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a660c5ad",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "scaling_sweep_task",
      "metadata": {},
      "source": [
        "## Q8. Scaling rules sweep\n",
        "\n",
        "Here, we pick:\n",
        " - $D_{\\text{mid}} = 1{,}250{,}000$  (tokens)\n",
        "-  $N_{\\text{mid}} = 16$            (model scale proxy)\n",
        "\n",
        "We now sweep around a midpoint $(D_{\\text{mid}}, N_{\\text{mid}})$ and scan along (approximate) **compute iso-FLOPs** to estimate scaling rules.\n",
        "\n",
        "We will build multiple groups indexed by $q$ (a coarse scale factor) so that $(D_q,N_q)=(q\\times D_{\\text{mid}}, q\\times N_{\\text{mid}})$. Here we pick $q=10^{1/4}$ to span multiplicative factors from 0.1 to 10. For each group, we generate several $(N, D)$ pairs by trading off dataset size $D$ and model scale $N$ while keeping the **total compute** controlled.\n",
        "\n",
        "For the learning rate, we use the rule fitted above:\n",
        "$$\n",
        "\\eta^\\star(C) = a_{\\mathrm{lr}}\\,C^{\\,b_{\\mathrm{lr}}}.\n",
        "$$\n",
        "\n",
        "To move along an iso-FLOP, we apply a multiplier \\(m\\) such that:\n",
        "$$\n",
        "D \\leftarrow D_q \\cdot m,\n",
        "\\qquad\n",
        "N \\leftarrow int(\\frac{N_q}{m}),\n",
        "$$\n",
        "which keeps $N \\cdot D$ roughly constant.\n",
        "\n",
        "Perform such parameter sweep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scaling_sweep_bash",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "84b98a40",
      "metadata": {},
      "source": [
        "Represent the isoFLOPS plots of the corresponding experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8e606fe",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fit_scaling_task",
      "metadata": {},
      "source": [
        "## Q9 — Scaling laws\n",
        "\n",
        "Using the sweeps, estimate scaling laws:\n",
        "\n",
        "- **Model size vs. compute:**  \n",
        "  $$\n",
        "  N = a_N\\, C^{b_N}\n",
        "  $$\n",
        "- **Data size vs. compute:**  \n",
        "  $$\n",
        "  D = a_D\\, C^{b_D}\n",
        "  $$\n",
        "\n",
        "where the compute proxy is:\n",
        "$$\n",
        "C = N \\cdot D\n",
        "$$\n",
        "\n",
        "Also fit the loss–compute relationship:\n",
        "$$\n",
        "L = a_L\\, C^{b_L}\n",
        "$$\n",
        "\n",
        "Use **one best point per** `q` **group**. You may use either:\n",
        "\n",
        "- `scipy.optimize.minimize`, or\n",
        "- `np.linalg.lstsq`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "collect_sweep_best_per_q",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "target_budget_task",
      "metadata": {},
      "source": [
        "## Q10 — Manipulation of the Scaling Law\n",
        "\n",
        "Run an experiment with a budget **$10 \\times$ larger** than the largest budget used in the previous parameter sweep. How would you pick the hyper-parameters?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "recommend_for_C10",
      "metadata": {},
      "outputs": [],
      "source": [
        "C0 = 1250000 * 16 * 100\n",
        "\n",
        "print(\"Target C:\", C0)\n",
        "print(\"Recommended D:\", D_int)\n",
        "print(\"Recommended N:\", N_int)\n",
        "print(\"Recommended lr:\", lr_hat)\n",
        "print(\"Predicted loss (rough):\", pred_L)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run_final_bash",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "source ./nanoGPTwrapper.sh\n",
        "\n",
        "ROOT=\"out/final/tinystories_$(date +%Y%m%d_%H%M%S)\"\n",
        "\n",
        "# run_nanogpt_job <lr> <D> <N> <root> <job_idx>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "final_plot_task",
      "metadata": {},
      "source": [
        "## Q11 — Conclusion\n",
        "\n",
        "Finally, plot the previous **final large-budget run** together with the **fitted curve(s)** on the same figure. Discuss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "final_plot_all",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
