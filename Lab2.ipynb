{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0b9e6d3c",
      "metadata": {},
      "source": [
        "# nanoGPT in Torchtitan (Shakespeare-char)\n",
        "__Designed by Edouard Oyallon__\n",
        "\n",
        "This notebook guides you through adding a **minimal `nanogpt` model** to torchtitan and reproducing the classic **tiny Shakespeare (char-level)** experiment. Refer to nanoGPT’s source code as the implementation reference.\n",
        "\n",
        "You will:\n",
        "- run `torchtitan`'s config\n",
        "- add a `shakespeare_char` dataset loader + random-block iterable dataset\n",
        "- add a nanoGPT-like character tokenizer\n",
        "- add a minimal nanoGPT model under `torchtitan/models/nanogpt/`\n",
        "- add nanoGPT-style optimizer grouping (decay only for 2D params)\n",
        "- add the compilation technique\n",
        "- reproduce a mini-gpt config\n",
        "- use wandb to verify your results and compare them with `nanogpt`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aa7cf75",
      "metadata": {},
      "source": [
        "## 1) Clone TorchTitan (or reuse existing)\n",
        "\n",
        "If you already have a `torchtitan/` folder, you can skip cloning.\n",
        "\n",
        "> TorchTitan often expects a recent PyTorch nightly when using bleeding-edge distributed/float8 features.\n",
        "For this lab (single GPU debug), you can often proceed with stable PyTorch too — but if your checkout requires nightly, install it as in their README.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0ce5a88",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "# If torchtitan already exists, do nothing\n",
        "if [ ! -d torchtitan ]; then\n",
        "  git clone https://github.com/pytorch/torchtitan.git\n",
        "fi\n",
        "\n",
        "cd torchtitan"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e77f0d31",
      "metadata": {},
      "source": [
        "## 2) Inspect `run_train.sh` and run the provided debug config\n",
        "\n",
        "`torchtitan` uses a config-first engine (TOML + CLI overrides).\n",
        "\n",
        "### How to change variables in `run_train.sh`\n",
        "\n",
        "Typically by setting env vars before calling it:\n",
        "- `NGPU=1`\n",
        "- `CONFIG_FILE=...`\n",
        "\n",
        "Let's run the official Llama3 debug config on 1 GPU. Explore the config file and train.py.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6625ae93",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "cd torchtitan\n",
        "\n",
        "ls -la run_train.sh\n",
        "sed -n '1,200p' run_train.sh\n",
        "\n",
        "NGPU=1 CONFIG_FILE=\"./torchtitan/models/llama3/train_configs/debug_model.toml\" ./run_train.sh\n",
        "\n",
        "cd ..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d9796cc",
      "metadata": {},
      "source": [
        "The structure of our new folder will be given by\n",
        "\n",
        "```text\n",
        "torchtitan/\n",
        "  torchtitan/\n",
        "    models/\n",
        "      nanogpt/\n",
        "        __init__.py # registry file\n",
        "        infra/\n",
        "          parallelize.py\n",
        "        model/\n",
        "          __init__.py\n",
        "          args.py\n",
        "          model.py\n",
        "        train_configs/\n",
        "          debug_model.toml\n",
        "          mini-gpt.toml\n",
        "```\n",
        "\n",
        "Look over the folder of llama3 to make sure you understand this structure. In particular, check the registry file.\n",
        "\n",
        "\n",
        "\n",
        "## 3) Add Shakespeare-char dataset loader + random-block dataset\n",
        "\n",
        "We will:\n",
        "- register a new dataset name (e.g. `shakespeare_char_train` / `shakespeare_char_validation`)\n",
        "- implement a data loader that downloads tiny Shakespeare text\n",
        "- tokenize it (later) and then sample nanoGPT-style random windows via `RandomBlockTokenDataset`.\n",
        "\n",
        "We implement these in `torchtitan/hf_datasets/text_datasets.py`. Modify the function ``build_text_dataloader`` to handle separately our new dataloader, whose codes are given below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd4e0b67",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _load_shakespeare_char_dataset(dataset_path, split: str):\n",
        "    \"\"\"Load Shakesperea char dataset with default configuration.\n",
        "       Inspired by karpathy/tiny_shakespeare.\n",
        "    \"\"\"\n",
        "\n",
        "    import requests\n",
        "    from datasets import Dataset, DatasetDict\n",
        "\n",
        "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "    text = requests.get(url).text\n",
        "\n",
        "    i = int(len(text) * 0.9)\n",
        "    train_text, val_text = text[:i], text[i:]\n",
        "    \n",
        "    ds = DatasetDict({\n",
        "        \"train\": Dataset.from_dict({\"text\": [train_text]}),\n",
        "        \"validation\": Dataset.from_dict({\"text\": [val_text]})\n",
        "    })\n",
        "    return ds[split]\n",
        "\n",
        "class RandomBlockTokenDataset(IterableDataset, Stateful):\n",
        "    \"\"\"\n",
        "    nanoGPT-style: sample random contiguous blocks from one long token sequence.\n",
        "    Yields single examples; DataLoader batching gives you local_batch_size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokens: torch.Tensor,          # 1D LongTensor\n",
        "        seq_len: int,\n",
        "        dp_rank: int = 0,\n",
        "        seed: int = 64,\n",
        "        infinite: bool = True,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        assert tokens.dim() == 1\n",
        "        assert tokens.dtype == torch.long\n",
        "        assert len(tokens) > seq_len + 1\n",
        "\n",
        "        self.tokens = tokens\n",
        "        self.seq_len = seq_len\n",
        "        self.infinite = infinite\n",
        "\n",
        "        # Per-rank RNG so different DP ranks sample different windows\n",
        "        self.seed = int(seed)\n",
        "        self.dp_rank = int(dp_rank)\n",
        "        self._steps = 0  # for checkpointing\n",
        "\n",
        "    def __iter__(self):\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(self.seed + self.dp_rank)\n",
        "\n",
        "        max_start = len(self.tokens) - (self.seq_len + 1)\n",
        "\n",
        "        while True:\n",
        "            # draw one random start index\n",
        "            i = int(torch.randint(0, max_start, (1,), generator=g).item())\n",
        "            x = self.tokens[i : i + self.seq_len]\n",
        "            y = self.tokens[i + 1 : i + 1 + self.seq_len]\n",
        "            self._steps += 1\n",
        "            yield {\"input\": x}, y\n",
        "\n",
        "            if not self.infinite:\n",
        "                break\n",
        "\n",
        "    def state_dict(self):\n",
        "        return {\"steps\": self._steps}\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self._steps = int(state_dict.get(\"steps\", 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6353ceec",
      "metadata": {},
      "source": [
        "### Quick registry test\n",
        "\n",
        "If `torchtitan` uses `DATASETS[...]` in that file, this should allow:\n",
        "\n",
        "```\n",
        "./run_train.sh --training.dataset shakespeare_char_train\n",
        "```\n",
        "\n",
        "If this override does not work immediately, revise your adaptation of the code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7de33d2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "cd torchtitan\n",
        "\n",
        "NGPU=1 CONFIG_FILE=\"./torchtitan/models/llama3/train_configs/debug_model.toml\" ./run_train.sh --training.dataset shakespeare_char_train\n",
        "\n",
        "cd ..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71b7b1c6",
      "metadata": {},
      "source": [
        "## 4) Add a nanoGPT-style character tokenizer\n",
        "\n",
        "We create `NanoGPTCharTokenizer` and a small builder function.\n",
        "\n",
        "- vocab = `sorted(set(text))`\n",
        "- encode: map each char to id\n",
        "- decode: map id to char\n",
        "- no BOS/EOS inserted\n",
        "\n",
        "File: `torchtitan/components/tokenizer/nanogpt_char_tokenizer.py`. Adapt the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0f06c1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class NanoGPTCharTokenizer(BaseTokenizer):\n",
        "    \"\"\"\n",
        "    Character-level tokenizer that matches nanoGPT/data/shakespeare_char/prepare.py:\n",
        "      - vocab = sorted(set(full_text))\n",
        "      - ids = enumerate(vocab)\n",
        "      - encode ignores bos/eos so TorchTitan won't inject extra tokens\n",
        "    \"\"\"\n",
        "    def __init__(self, url: str,):\n",
        "        super().__init__()\n",
        "        import requests\n",
        "        text = requests.get(url).text\n",
        "\n",
        "        chars = sorted(set(text))  # matches nanoGPT's chars = sorted(list(set(data)))\n",
        "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "        self.itos = {i: ch for ch, i in self.stoi.items()}\n",
        "        self._n_words = len(chars)\n",
        "\n",
        "        # TorchTitan tokenizers often expose these attrs; keep them defined.\n",
        "        self.bos_id = -1 # that's the token for the beginning of sequence\n",
        "        self.eos_id = self._n_words # that's the token for the end of sequence\n",
        "\n",
        "    def encode(self, *args, **kwargs) -> list[int]:\n",
        "        # Extract arguments\n",
        "        if len(args) >= 1:\n",
        "            text = args[0]\n",
        "        else:\n",
        "            text = kwargs.get(\"text\", \"\")\n",
        "       ...\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, *args, **kwargs) -> str:\n",
        "        \"\"\"\n",
        "        Decode token IDs back to text.\n",
        "\n",
        "        Args:\n",
        "            token_ids (list[int]): List of token IDs to decode\n",
        "            **kwargs: Additional arguments passed to the underlying tokenizer's decode method\n",
        "                     (e.g., skip_special_tokens)\n",
        "\n",
        "        Returns:\n",
        "            str: Decoded text\n",
        "        \"\"\"\n",
        "        # Extract token_ids from arguments\n",
        "        if len(args) >= 1:\n",
        "            token_ids = args[0]\n",
        "            # Pass through remaining kwargs\n",
        "            return ...\n",
        "        else:\n",
        "            token_ids = kwargs.pop(\"token_ids\", [])\n",
        "            # Pass through remaining kwargs after removing token_ids\n",
        "            return ...\n",
        "    \n",
        "    def get_vocab_size(self) -> int:\n",
        "        return self._n_words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7f2f32a",
      "metadata": {},
      "source": [
        "## 5) Add nanoGPT-style optimizer grouping (decay only on 2D tensors)\n",
        "\n",
        "nanoGPT applies weight decay only to parameters that are **matrix-shaped** (dim >= 2) (verify this in `nanogpt` implementation), and excludes biases/LayerNorm vectors.\n",
        "\n",
        "Implement:\n",
        "- `_NanoGPTDecayByDimMixin`\n",
        "- `NanoGPTAdamW`\n",
        "- `NanoGPTAdam`\n",
        "\n",
        "Then, patch `torchtitan/components/optimizer.py` by appending classes at the end and adapting the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b05a8b62",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class _NanoGPTDecayByDimMixin:\n",
        "    def __init__(self, params, *args, weight_decay: float = 0.0, **kwargs):\n",
        "        params = [p for p in params if p.requires_grad]\n",
        "\n",
        "        decay = [p for p in params if ...]\n",
        "        nodecay = [p for p in params if ...]\n",
        "        # adapt the reste of the code\n",
        "        super().__init__(groups, *args, **kwargs)\n",
        "\n",
        "class NanoGPTAdamW(_NanoGPTDecayByDimMixin, torch.optim.AdamW):\n",
        "    pass\n",
        "\n",
        "\n",
        "class NanoGPTAdam(_NanoGPTDecayByDimMixin, torch.optim.Adam):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a5e8b54",
      "metadata": {},
      "source": [
        "## 6) Implement `nanogpt/model.py` (port nanoGPT to TorchTitan)\n",
        "\n",
        "Adapt nanoGPT’s GPT model to TorchTitan’s model structure, keeping the code **minimal and clean**. We add:\n",
        "- `torchtitan/models/nanogpt/model/args.py` — dataclass config\n",
        "- `torchtitan/models/nanogpt/model/model.py` — minimal GPT\n",
        "- `debug_model.toml`: very small, fast sanity check\n",
        "- init files\n",
        "\n",
        "**References**\n",
        "- nanoGPT source: https://github.com/karpathy/nanoGPT/blob/master/model.py#L118  \n",
        "- `torchtitan` style example (Llama3): https://github.com/pytorch/torchtitan/blob/main/torchtitan/models/llama3/model/model.py\n",
        "\n",
        "### Requirements\n",
        "- **No clutter:** avoid unnecessary helper functions; remove unused code.\n",
        "- **Rename:** use `self.layers` instead of `self.transformer`.\n",
        "- **Initialization:** add `init_weights()`, which initializes every parameters. Every submodule (Attention/MLP/Block/Model) must implement its own `init_weights()` and is responsible to initialize its own submodules.\n",
        "- **Device:** don’t store a custom device; derive it from a parameter/buffer (e.g., embedding weight device).\n",
        "- **Forward contract:** input format similar to TorchTitan models; output **logits only**.  \n",
        "  Remove nanoGPT’s extra “optimization” logic in the forward.\n",
        "- **Layer container:** replace `nn.ModuleList([...])` with a **named** container (e.g. `nn.ModuleDict`) so layers are trackable for transformations (e.g. activation checkpointing).\n",
        "\n",
        "### Integration + test\n",
        "- Export/register the model: update `torchtitan/models/__init__.py` (and local `nanogpt/__init__.py`).\n",
        "- Test using a **modified Llama3 debug config**, but keep **dataset = C4** for now.\n",
        "- Ensure `vocab_size` matches the active tokenizer.\n",
        "- For this lab phase set:\n",
        "  - `parallelize_fn = lambda m, *args, **kwargs: m`\n",
        "  - `pipelining_fn = None`\n",
        "- Propose several configurations which match what is in nanoGPT.\n",
        "\n",
        "Test and a debug an initial version.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfad5a58",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "cd torchtitan/\n",
        "NGPU=1 CONFIG_FILE=\"./torchtitan/models/nanogpt/train_configs/debug_model.toml\" ./run_train.sh\n",
        "cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2b66bd2",
      "metadata": {},
      "source": [
        "## 7) Add minimal `nanogpt` train configs\n",
        "\n",
        "Create:\n",
        "\n",
        "- `mini-gpt.toml`: using nanoGPT train_shakespeare_char defaults\n",
        "\n",
        "Test your model on a few iterations to make sure it runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3c38e73",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45668899",
      "metadata": {},
      "source": [
        "## 8) Compare both implementations (TorchTitan vs nanoGPT)\n",
        "\n",
        "- Update **nanoGPT** so it logs the **training loss at every iteration** (same frequency as TorchTitan).\n",
        "- Run **both** experiments with **matched hyperparameters** (same config as much as possible).\n",
        "- Use **Weights & Biases** to compare:\n",
        "  - training loss curve\n",
        "  - validation loss curve\n",
        "  - learning rate schedule\n",
        "\n",
        "If the curves don’t match closely, re-check the usual suspects:\n",
        "- learning rate (and warmup/decay schedule)\n",
        "- batch size / gradient accumulation\n",
        "- sequence length / block size\n",
        "- weight decay and optimizer settings (decay grouping)\n",
        "- model initialization (std, tied weights, bias flags)\n",
        "- tokenizer / vocabulary size and dataset split\n",
        "- dropout / seed / randomness\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5f39df",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "WANDB_PROJECT=\"shakespeare-char\" WANDB_NAME=\"torchtitan-mini-gpt\" \\\n",
        "  NGPU=1 CONFIG_FILE=\"./torchtitan/models/nanogpt/train_configs/mini-gpt.toml\" \\\n",
        "  ./run_train.sh --metrics.enable-wandb\n",
        "\n",
        "python train.py config/train_shakespeare_char.py   --wandb_log=True "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14038f5f",
      "metadata": {},
      "source": [
        "## 9) Load a Hugging Face checkpoint and run validation\n",
        "\n",
        " Load a pretrained nanoGPT checkpoint from Hugging Face into TorchTitan (via **DCP**) and verify that TorchTitan reports  **a similar validation loss** as nanoGPT on Shakespeare-char.\n",
        "\n",
        "> **DCP** = PyTorch Distributed Checkpointing (`torch.distributed.checkpoint`), suitable for sharded checkpoints (FSDP/TP/PP).\n",
        "\n",
        "Inspect on Hugginface sosier/nanoGPT-shakespeare-char-tied-weights. Download the model and convert it to DCP using the script below.\n",
        "\n",
        "```bash\n",
        "hf download sosier/nanoGPT-shakespeare-char-tied-weights --local-dir hf_model\n",
        "```\n",
        "\n",
        "Run the validation test and follow the same load-only + validate procedure as in the previous step, and compare the validation losses between nanoGPT and `torchtitan`. They should closely match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd8e33a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.distributed.checkpoint as dcp\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--in_bin\", required=True)\n",
        "    ap.add_argument(\"--out_dcp\", required=True)\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    sd = torch.load(args.in_bin, map_location=\"cpu\")\n",
        "    dcp.save(sd, checkpoint_id=args.out_dcp)\n",
        "    print(\"Wrote DCP checkpoint to\", args.out_dcp)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c05f4e69",
      "metadata": {},
      "outputs": [],
      "source": [
        "WANDB_PROJECT=\"shakespeare-char\" WANDB_NAME=\"torchtitan-mini-gpt\" \\\n",
        "NGPU=1 CONFIG_FILE=\"./torchtitan/models/nanogpt/train_configs/mini-gpt.toml\" \\\n",
        "./run_train.sh \\\n",
        "  --training.steps 1 \\\n",
        "  --metrics.no-enable-wandb \\\n",
        "  --checkpoint.enable \\\n",
        "  --checkpoint.load_only \\\n",
        "  --checkpoint.initial_load_path ../hf_model/nanogpt_char_dcp \\\n",
        "  --validation.freq 1 \\\n",
        "  --optimizer.lr 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0f71a5",
      "metadata": {},
      "source": [
        "## 10) Compile\n",
        "\n",
        "Take inspiration from the Llama3 infrastructure in `infra/parallelize`. Add a **nanoGPT-specific** `parallelize` function whose only job is to:\n",
        "\n",
        "- **compile the model** (e.g. via `torch.compile`)\n",
        "- **return the compiled model**\n",
        "- **raise an error** if any other parallelization mode is requested (TP/PP/FSDP/activation checkpointing, etc.)\n",
        "\n",
        "To do so, implement `parallelize_nanogpt(...)` in the appropriate `infra/parallelize` module. Run the nanoGPT with compilation. What should we think of the MFU compared to nanoGPT?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4dd73bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "cd torchtitan\n",
        "\n",
        "WANDB_PROJECT=\"shakespeare-char\" WANDB_NAME=\"torchtitan-mini-gpt\" NGPU=1 CONFIG_FILE=\"./torchtitan/models/nanogpt/train_configs/mini-gpt.toml\" ./run_train.sh  --compile.enable"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
